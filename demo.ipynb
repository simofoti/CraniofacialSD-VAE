{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Demo of \"Latent Disentanglement in Mesh Variational Autoencoders Improves the Diagnosis of Craniofacial Syndromes and Aids Surgical Planning\"\n",
    "\n",
    "### Simone Foti, Alexander J. Rickart, Bongjin Koo, Eimear Oâ€™ Sullivan, Lara S. van de Lande, Athanasios Papaioannou, Roman Khonsari, Danail Stoyanov, N. u. Owase Jeelani, Silvia Schievano, David J. Dunaway, Matthew J. Clarkson\n",
    "\n",
    "Before running this notebook, make sure you have followed the installation instructions detailed in the README.md file.\n",
    "\n",
    "----\n",
    "\n",
    "## Import all necessary libraries and initialise the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import trimesh\n",
    "import torch\n",
    "import scipy.stats\n",
    "import scipy.linalg\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "from model_manager import ModelManager\n",
    "\n",
    "\n",
    "demo_directory = \"demo_files\"\n",
    "configurations = utils.get_config(os.path.join(demo_directory, \"config.yaml\"))\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, running on CPU\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "manager = ModelManager(\n",
    "    configurations=configurations, device=device,\n",
    "    precomputed_storage_path=configurations['data']['precomputed_path'])\n",
    "manager.resume(os.path.join(demo_directory, \"checkpoints\"))\n",
    "manager.set_class_conversions({'a': 0, 'm': 1, 'c': 2, 'n': 3, 'b': 4})  # b and n are merged for classification\n",
    "\n",
    "label2name_dict = {'a': \"Apert\", 'b': \"Healthy\", 'c': \"Crouzon\", \n",
    "                   'm': \"Muenke\", 'n': \"Healthy\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the demo meshes\n",
    "Note that these meshes are not from real subjects and were obtained with our data augmentation based on spectral interpolation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_meshes = []\n",
    "demo_meshes_labels = []\n",
    "meshes_directory = os.path.join(demo_directory, \"meshes\")\n",
    "for dirpath, _, fnames in os.walk(meshes_directory):\n",
    "    for f in fnames:\n",
    "        if f.endswith('.ply') or f.endswith('.obj'):\n",
    "            mesh_path = os.path.join(meshes_directory, f)\n",
    "            demo_meshes.append(trimesh.load_mesh(mesh_path, process=False))\n",
    "            demo_meshes_labels.append(f[0])\n",
    "\n",
    "print(f\"{len(demo_meshes)} meshes available. Use indices between 0 and\",\n",
    "      f\"{len(demo_meshes) - 1} to select them\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to visualize the different meshes. Change the value of `mesh_id` according to the values specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_id = 0\n",
    "\n",
    "print(f\"mesh class: {label2name_dict[demo_meshes_labels[mesh_id]]}\")\n",
    "demo_meshes[mesh_id].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of data augmentation\n",
    "\n",
    "Running the eigendecomposition may take up to a few minutes depending on the computer running the code. For this reason we reccomend to run the next cell once and then experiment with the data augmentation in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigd = utils.compute_laplacian_eigendecomposition(manager.template, k=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the value of `mesh1_id` and `mesh2_id` to test the data augmentation with different mesh pairs. Note that in this demo example you can create augmented samples even from subjects in different age groups and with different syndromes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh1_id = 1\n",
    "mesh2_id = 3\n",
    "\n",
    "mesh1_class = label2name_dict[demo_meshes_labels[mesh1_id]]\n",
    "mesh2_class = label2name_dict[demo_meshes_labels[mesh2_id]]\n",
    "assert mesh1_id != mesh2_id\n",
    "if mesh1_class != mesh2_class: \n",
    "    print(\"The selected meshes come from different classes:\",\n",
    "          f\"{mesh1_class} and {mesh2_class}.\"\n",
    "          \"You should select meshes with the same class\")\n",
    "\n",
    "mesh1 = demo_meshes[mesh1_id].copy()\n",
    "mesh2 = demo_meshes[mesh2_id].copy()\n",
    "x1 = np.array(mesh1.vertices)\n",
    "x2 = np.array(mesh2.vertices)\n",
    "\n",
    "x_aug = utils.spectral_interpolation(x1, x2, eigd)\n",
    "mesh_aug = demo_meshes[mesh1_id].copy()\n",
    "mesh_aug.vertices = x_aug\n",
    "\n",
    "print(\"Mesh 1 depicted on the left, \\nMesh 2 on the right, \\nAugmented mesh in the middle\")\n",
    "scene = trimesh.scene.scene.Scene()\n",
    "mesh1.vertices[:, 0] = mesh1.vertices[:, 0] - 2\n",
    "scene.add_geometry(mesh1)\n",
    "mesh2.vertices[:, 0] = mesh2.vertices[:, 0] + 2\n",
    "scene.add_geometry(mesh2)\n",
    "scene.add_geometry(mesh_aug)\n",
    "trimesh.scene.lighting.autolight(scene)\n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnose demo meshes\n",
    "\n",
    "#### Patient classification\n",
    "\n",
    "The mesh is encoded, with SD-VAE and the latent vector is classified with QDA. See different classification results by changing `mesh_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_id = 2\n",
    "\n",
    "normalization_dict_path = os.path.join(demo_directory, \"norm.pt\")\n",
    "normalization_dict = torch.load(normalization_dict_path)\n",
    "\n",
    "mesh_class = label2name_dict[demo_meshes_labels[mesh_id]]\n",
    "mesh_verts = torch.tensor(demo_meshes[mesh_id].vertices, \n",
    "                          dtype=torch.float,\n",
    "                          requires_grad=False, device='cpu')\n",
    "v_p = (mesh_verts - normalization_dict['mean']) / normalization_dict['std']\n",
    "z_p = manager.encode(v_p.unsqueeze(0).to(device))\n",
    "classification_result = label2name_dict[manager.classify_latent(z_p, 'qda')[0]]\n",
    "\n",
    "print(f\"The selected mesh was a {mesh_class} patient\") \n",
    "print(f\"It was classified as a {classification_result} patient\")\n",
    "\n",
    "demo_meshes[mesh_id].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project on global manifold visualisation\n",
    "\n",
    "The 75-dimensional latent vector corresponding to the patient's head is projected with LDA. Since the latent distribution plot is obtained relying on the entire dataset, here we load a precomputed plot and project new samples on it. \n",
    "\n",
    "If you want to see the plot of a different subject, change `mesh_id` in the **patient classification** section and run that cell before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "fig_entire_z_name = os.path.join(demo_directory, \"lda_emb_distributions.pkl\")\n",
    "\n",
    "with open(fig_entire_z_name, 'rb') as f:\n",
    "    fig_entire_z = pickle.load(f)\n",
    "\n",
    "z_proj = manager.lda_project_latents_in_2d(z_p.detach().cpu().numpy())\n",
    "\n",
    "ax = fig_entire_z.gca()\n",
    "sns.scatterplot(x=z_proj[:, 0], y=z_proj[:, 1], ax=ax, c=['#e881a7'])\n",
    "fig_entire_z.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project on local manifold visualisations\n",
    "\n",
    "Similarly to the global manifold visualisation, also the local distribution plots are precomputed. The projections are performed with the pre-trained attribute-specific LDA models. Each LDA model projects a 5-dimensional latent vector in a 2-dimensional space.\n",
    "\n",
    "If you want to see the plot of a different subject, change `mesh_id` in the **patient classification** section and run that cell before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_regions_z_name = os.path.join(demo_directory, \"emb_all_train_dist.pkl\")\n",
    "region_ldas_name = os.path.join(demo_directory, \"region_ldas.pkl\")\n",
    "\n",
    "with open(fig_regions_z_name, 'rb') as f:\n",
    "    fig_fgrid_regions_z = pickle.load(f)\n",
    "with open(region_ldas_name, 'rb') as f:\n",
    "    region_ldas = pickle.load(f)\n",
    "    \n",
    "\n",
    "z_p_np = z_p.detach().cpu().numpy()\n",
    "r_proj = {}\n",
    "for key, z_region in manager.latent_regions.items():\n",
    "    z_p_region = z_p_np[:, z_region[0]:z_region[1]]\n",
    "    z_r_embeddings = region_ldas[key].transform(z_p_region)\n",
    "    r_proj[key] = z_r_embeddings\n",
    "    x1, x2 = z_r_embeddings[:, 0], z_r_embeddings[:, 1]\n",
    "    fig_fgrid_regions_z.axes_dict[utils.colour2attribute_dict[key]].scatter(\n",
    "        x1, x2, c=['#e881a7'], s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surgical planning of demo meshes\n",
    "\n",
    "Running the first cell will display the keywords associated to the different surgical procedures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Use the following keys to select a procedure in the next cell:\")\n",
    "print(list(utils.procedures2attributes_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a patient and a procedure to start the surgical planning. This can be done by modifying the `mesh_id` and `procedure_id` variables in the next code cell. Procedures' keywords are reported above. \n",
    "\n",
    "#### Global interpolation trajectory\n",
    "The following cell will project the different interpolation steps on the plot depicting the latent distributions of the whole latent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_id = 0\n",
    "procedure_id = 'monobloc'\n",
    "\n",
    "def vector_linspace(start, finish, steps):\n",
    "    ls = []\n",
    "    for s, f in zip(start[0], finish[0]):\n",
    "        ls.append(torch.linspace(s, f, steps))\n",
    "    res = torch.stack(ls)\n",
    "    return res.t()\n",
    "\n",
    "mesh_class = label2name_dict[demo_meshes_labels[mesh_id]]\n",
    "assert mesh_class != \"Healthy\", \"The patient is already healthy!\"\n",
    "\n",
    "mesh_verts = torch.tensor(demo_meshes[mesh_id].vertices, \n",
    "                          dtype=torch.float,\n",
    "                          requires_grad=False, device='cpu')\n",
    "v_p = (mesh_verts - normalization_dict['mean']) / normalization_dict['std']\n",
    "z_p = manager.encode(v_p.unsqueeze(0).to(device))\n",
    "\n",
    "# Find healthy patients latent vectors\n",
    "normal_p_index = manager.class2idx('n')\n",
    "normal_p_mean = manager.qda.means_[normal_p_index]\n",
    "\n",
    "# Move from mean of distribution to 1std in direction of z_p.\n",
    "# Eigenvalues of covariance matrix are diagonal of covariance of aligned\n",
    "# distribution -> use them to find pdf at 1 std\n",
    "normal_p_covariance = manager.qda.covariance_[normal_p_index]\n",
    "multi_normal_dist = scipy.stats.multivariate_normal(\n",
    "    mean=normal_p_mean, cov=normal_p_covariance)\n",
    "eigenval, eigenvec = scipy.linalg.eigh(normal_p_covariance)\n",
    "reference_dist = scipy.stats.multivariate_normal(\n",
    "    mean=np.zeros_like(normal_p_mean), cov=np.diag(eigenval))\n",
    "reference_std_on_x1 = np.sqrt(reference_dist.cov[0, 0])\n",
    "reference_std_vec_on_x1 = np.zeros_like(normal_p_mean)\n",
    "reference_std_vec_on_x1[0] = reference_std_on_x1\n",
    "\n",
    "reference_pdf_1std = - reference_dist.logpdf(reference_std_vec_on_x1)\n",
    "reference_pdf_2std = - reference_dist.logpdf(2 * reference_std_vec_on_x1)\n",
    "reference_pdf_3std = - reference_dist.logpdf(3 * reference_std_vec_on_x1)\n",
    "\n",
    "z_mean_target = torch.tensor(normal_p_mean).unsqueeze(0)\n",
    "z_interp_full = vector_linspace(z_p, z_mean_target, 5000)\n",
    "\n",
    "# find z vectors with correct pdf\n",
    "pdf_intermediate = [-multi_normal_dist.logpdf(z.detach().cpu().numpy())\n",
    "                    for z in z_interp_full]\n",
    "pdf_lt_3std = [p <= reference_pdf_3std for p in pdf_intermediate]\n",
    "pdf_lt_2std = [p <= reference_pdf_2std for p in pdf_intermediate]\n",
    "pdf_lt_1std = [p <= reference_pdf_1std for p in pdf_intermediate]\n",
    "\n",
    "z_3std_target = z_interp_full[pdf_lt_3std.index(True), :].unsqueeze(0)\n",
    "z_2std_target = z_interp_full[pdf_lt_2std.index(True), :].unsqueeze(0)\n",
    "z_1std_target = z_interp_full[pdf_lt_1std.index(True), :].unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "# Interpolate subsets of attributes ####################################\n",
    "\n",
    "attributes = utils.procedures2attributes_dict[procedure_id]\n",
    "n_p_to_3std = 8\n",
    "z_interp = z_p.repeat(n_p_to_3std + 3, 1)\n",
    "for attr in attributes:\n",
    "    zf_idxs = manager.latent_regions[attr]\n",
    "    z_pf = z_p[:, zf_idxs[0]:zf_idxs[1]].to(device)\n",
    "    z_3f = z_3std_target[:, zf_idxs[0]:zf_idxs[1]].to(device)\n",
    "    z_interp[:n_p_to_3std, zf_idxs[0]:zf_idxs[1]] = \\\n",
    "        vector_linspace(z_pf, z_3f, n_p_to_3std)\n",
    "    z_2f = z_2std_target[:, zf_idxs[0]:zf_idxs[1]].to(device)\n",
    "    z_1f = z_1std_target[:, zf_idxs[0]:zf_idxs[1]].to(device)\n",
    "    z_mf = z_mean_target[:, zf_idxs[0]:zf_idxs[1]].to(device)\n",
    "    z_interp[n_p_to_3std, zf_idxs[0]:zf_idxs[1]] = z_2f\n",
    "    z_interp[n_p_to_3std + 1, zf_idxs[0]:zf_idxs[1]] = z_1f\n",
    "    z_interp[n_p_to_3std + 2, zf_idxs[0]:zf_idxs[1]] = z_mf\n",
    "\n",
    "\n",
    "z_interp_proj = manager.lda_project_latents_in_2d(z_interp.detach().cpu().numpy())\n",
    "\n",
    "with open(fig_entire_z_name, 'rb') as f:\n",
    "    fig_entire_z = pickle.load(f)\n",
    "ax = fig_entire_z.gca()\n",
    "sns.scatterplot(x=z_interp_proj[:, 0], y=z_interp_proj[:, 1], ax=ax, c=['#e881a7'])\n",
    "fig_entire_z.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local interpolation trajectories\n",
    "\n",
    "The following cell will project the different interpolation steps on the plots depicting the latent distributions of the 5D attribute-specific latent vectors.\n",
    "\n",
    "If you want to change patient or procedure modify the `mesh_id` and `procedure_id` variables in the **global interpolation trajectory** section. Then run all cells from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fig_regions_z_name, 'rb') as f:\n",
    "    fig_fgrid_regions_z = pickle.load(f)\n",
    "\n",
    "z_interp_np = z_interp.detach().cpu().numpy()\n",
    "r_proj = {}\n",
    "for key, z_region in manager.latent_regions.items():\n",
    "    z_interp_region = z_interp_np[:, z_region[0]:z_region[1]]\n",
    "    z_r_embeddings = region_ldas[key].transform(z_interp_region)\n",
    "    r_proj[key] = z_r_embeddings\n",
    "    x1, x2 = z_r_embeddings[:, 0], z_r_embeddings[:, 1]\n",
    "    fig_fgrid_regions_z.axes_dict[utils.colour2attribute_dict[key]].scatter(\n",
    "        x1, x2, c=['#e881a7'], s=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show results of latent interpolation as a static image\n",
    "\n",
    "Every rendered shape corresponds to one of the pink dots in the previous plots.\n",
    "\n",
    "If you want to change patient or procedure modify the `mesh_id` and `procedure_id` variables in the **global interpolation trajectory** section. Then run all cells from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from pytorch3d.renderer import BlendParams\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "manager.renderer.rasterizer.raster_settings.image_size = 512\n",
    "blend_params = BlendParams(background_color=[1, 1, 1])\n",
    "manager.default_shader.blend_params = blend_params\n",
    "manager.simple_shader.blend_params = blend_params\n",
    "\n",
    "v_interp = manager.generate(z_interp.to(device))\n",
    "v_interp = v_interp * normalization_dict['std'].to(device) + \\\n",
    "    normalization_dict['mean'].to(device)\n",
    "\n",
    "\n",
    "source_dist = manager.compute_vertex_errors(\n",
    "    v_interp, v_interp[0, ::].expand(v_interp.shape[0], -1, -1))\n",
    "source_colours = utils.errors_to_colors(\n",
    "    source_dist, min_value=0, max_value=10, cmap='plasma') / 255\n",
    "\n",
    "renderings = manager.render(v_interp).cpu()\n",
    "renderings_dist = manager.render(v_interp, source_dist, error_max_scale=10).cpu()\n",
    "\n",
    "im = make_grid(torch.cat([renderings, renderings_dist], dim=-2),\n",
    "               padding=10, pad_value=1, nrow=v_interp.shape[0])\n",
    "plt.figure()\n",
    "plt.imshow(np.asarray(to_pil_image(im)))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show results of latent interpolation as video\n",
    "\n",
    "Every frame corresponds to one of the pink dots in the previous plots.\n",
    "\n",
    "If you want to change patient or procedure modify the `mesh_id` and `procedure_id` variables in the **global interpolation trajectory** section. Then run all cells from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rend_comb = torch.cat([renderings, renderings_dist], dim=-1)\n",
    "fig = plt.figure()\n",
    "plt.axis('off')\n",
    "frames = [[plt.imshow(np.asarray(to_pil_image(rend_comb[i])), animated=True)]\n",
    "          for i in range(rend_comb.shape[0])]\n",
    "ani = matplotlib.animation.ArtistAnimation(\n",
    "    fig, frames, interval=200, blit=True, repeat_delay=500)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
